{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math # For SwiGLU\n",
    "\n",
    "# --- Helper Modules ---\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    SwiGLU activation function module, commonly used in Transformer MLPs.\n",
    "    Based on https://arxiv.org/pdf/2002.05202.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, bias=False):\n",
    "        super().__init__()\n",
    "        self.linear_gate = nn.Linear(dim_in, dim_out, bias=bias)\n",
    "        self.linear_value = nn.Linear(dim_in, dim_out, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., dim_in)\n",
    "        gate = self.linear_gate(x)\n",
    "        value = self.linear_value(x)\n",
    "        # Element-wise multiplication of sigmoid(gate) * value\n",
    "        return F.silu(gate) * value # silu(x) = x * sigmoid(x)\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard MLP block found in Transformers, using SwiGLU.\n",
    "    Typically, hidden_mult=4, but can be adjusted.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, hidden_mult=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        dim_inner = hidden_dim * hidden_mult\n",
    "        # The paper suggests making the intermediate dim 2/3 of dim_inner for SwiGLU\n",
    "        # but we'll keep it simple here. Adjust if needed.\n",
    "        # dim_inner_glu = int(2 * dim_inner / 3)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, dim_inner * 2, bias=False) # Combined for SwiGLU gate/value\n",
    "        self.act = nn.SiLU() # SwiGLU uses SiLU (Sigmoid Linear Unit) internally\n",
    "        self.fc2 = nn.Linear(dim_inner, hidden_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Project up and split for SwiGLU parts\n",
    "        gate_value = self.fc1(x).chunk(2, dim=-1)\n",
    "        # Apply SwiGLU logic: silu(gate) * value\n",
    "        hidden = self.act(gate_value[0]) * gate_value[1]\n",
    "        hidden = self.dropout(hidden)\n",
    "        # Project back down\n",
    "        output = self.fc2(hidden)\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TTT Components ---\n",
    "\n",
    "class TTTTask(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines the learnable projections for the multi-view reconstruction task\n",
    "    within the TTT layer, as described in Section 2.3 and Figure 5.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, projection_rank=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of the input/output features.\n",
    "            projection_rank (int, optional): If provided, uses low-rank projections.\n",
    "                                            Otherwise, uses full-rank projections.\n",
    "                                            Defaults to None (full-rank).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Use projection_rank if specified, otherwise use full hidden_dim\n",
    "        rank = projection_rank if projection_rank is not None else hidden_dim\n",
    "\n",
    "        # Learnable outer-loop parameters theta_K, theta_V, theta_Q\n",
    "        # These define the self-supervised task (Eq. 4) and output rule (Eq. 5)\n",
    "        self.proj_K = nn.Linear(hidden_dim, rank, bias=False) # Training view\n",
    "        self.proj_V = nn.Linear(hidden_dim, rank, bias=False) # Label view\n",
    "        self.proj_Q = nn.Linear(hidden_dim, rank, bias=False) # Test view\n",
    "\n",
    "        # The inner model 'f' will operate on dimensions of size 'rank'\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies the projections to create the different views.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (..., hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (x_train, x_label, x_test) tensors of shape (..., rank).\n",
    "        \"\"\"\n",
    "        x_train = self.proj_K(x) # Used as input to f for the inner loss\n",
    "        x_label = self.proj_V(x) # Used as the target for the inner loss\n",
    "        x_test  = self.proj_Q(x) # Used as input to f for the layer's output\n",
    "        return x_train, x_label, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a Test-Time Training (TTT) layer based on the paper.\n",
    "    Uses mini-batch updates (Section 2.4) and learnable components.\n",
    "    The inner model 'f' is a linear transformation with LayerNorm and Residual.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, inner_eta_base=0.1, ttt_batch_size=16, projection_rank=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_dim (int): Dimension of input features.\n",
    "            inner_eta_base (float): Base learning rate for the inner loop (eta_base in Sec 2.7).\n",
    "            ttt_batch_size (int): Mini-batch size for inner loop updates (b in Sec 2.4).\n",
    "            projection_rank (int, optional): Rank for the task projections. Defaults to hidden_dim.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.inner_eta_base = inner_eta_base\n",
    "        self.ttt_batch_size = ttt_batch_size\n",
    "        self.projection_rank = projection_rank if projection_rank is not None else hidden_dim\n",
    "\n",
    "        # Learnable multi-view task projections (theta_K, theta_V, theta_Q)\n",
    "        self.task = TTTTask(hidden_dim, projection_rank=self.projection_rank)\n",
    "\n",
    "        # Learnable initial weights for the inner linear model (theta_init = W_0 in Sec 2.7)\n",
    "        # The inner model maps rank -> rank\n",
    "        self.initial_W = nn.Parameter(torch.randn(self.projection_rank, self.projection_rank) * 0.01)\n",
    "\n",
    "        # Learnable inner learning rate components (theta_lr in Sec 2.7)\n",
    "        # Maps hidden_dim -> 1, output is used in sigmoid for gating eta\n",
    "        self.theta_lr = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        # LayerNorm for the inner model f (Sec 2.7)\n",
    "        self.inner_norm = nn.LayerNorm(self.projection_rank)\n",
    "\n",
    "    def _inner_model_f(self, view, W):\n",
    "        \"\"\"\n",
    "        Implements the inner model f(view; W) = view + LN(W @ view).\n",
    "        This applies the linear transformation, LayerNorm, and residual connection.\n",
    "\n",
    "        Args:\n",
    "            view (torch.Tensor): Input view (e.g., x_train or x_test), shape (..., rank).\n",
    "            W (torch.Tensor): Current inner model weights, shape (rank, rank).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output of the inner model, shape (..., rank).\n",
    "        \"\"\"\n",
    "        # Linear transformation: W maps rank -> rank\n",
    "        # view shape: (batch_size_token, rank)\n",
    "        # W shape: (rank, rank)\n",
    "        # W.t() shape: (rank, rank)\n",
    "        # result shape: (batch_size_token, rank)\n",
    "        transformed_view = F.linear(view, W) # Equivalent to torch.matmul(view, W.t())\n",
    "\n",
    "        # Apply LayerNorm and Residual connection\n",
    "        # output = view + self.inner_norm(transformed_view) # Paper: f(x) = x + LN(f_res(x))\n",
    "        # Let's double check Eq 4, 5. f is applied to the *view*.\n",
    "        # So the residual should be added to the *view*.\n",
    "        output = view + self.inner_norm(transformed_view)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Processes the input sequence using mini-batch TTT updates.\n",
    "\n",
    "        Args:\n",
    "            input_seq (torch.Tensor): Input sequence tensor of shape\n",
    "                                      (seq_len, batch_size, hidden_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output sequence tensor of shape\n",
    "                          (seq_len, batch_size, hidden_dim).\n",
    "                          Note: Output dim matches input dim, even though\n",
    "                          inner model works on 'rank'. We need a final projection.\n",
    "                          --> Let's rethink. The TTT layer should output hidden_dim.\n",
    "                          The inner model f outputs rank. How does it map back?\n",
    "                          The paper's Figures 1, 3, 5 show z_t = f(x_t; W_t).\n",
    "                          If f outputs rank, z_t has rank dim. This needs to feed\n",
    "                          into the next layer expecting hidden_dim.\n",
    "                          Possibility 1: The output rule f(theta_Q x_t; W_t) implicitly\n",
    "                                         includes a projection back to hidden_dim.\n",
    "                          Possibility 2: The TTT layer itself includes an output projection.\n",
    "                          Possibility 3: The *entire* hidden state of the RNN includes W\n",
    "                                         and maybe other things, and the output rule combines them.\n",
    "                          Let's assume the TTTLayer should output hidden_dim.\n",
    "                          We can add a learnable output projection nn.Linear(rank, hidden_dim)\n",
    "                          applied to the result of f(x_test, W_updated).\n",
    "\n",
    "                          Let's add an output projection.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, _ = input_seq.shape\n",
    "        device = input_seq.device\n",
    "\n",
    "        # Initialize inner model weights (W_0) - clone to avoid modifying parameter directly\n",
    "        W = self.initial_W.clone().detach().requires_grad_(True) # Start fresh for each sequence forward pass\n",
    "\n",
    "        outputs = []\n",
    "        hidden_states_W = [W] # Store W at the start of each mini-batch\n",
    "\n",
    "        # --- Mini-batch TTT Loop ---\n",
    "        for t_start in range(0, seq_len, self.ttt_batch_size):\n",
    "            t_end = min(t_start + self.ttt_batch_size, seq_len)\n",
    "            current_batch_size = t_end - t_start\n",
    "\n",
    "            # Get the current mini-batch of input tokens\n",
    "            # Shape: (current_batch_size, batch_size, hidden_dim)\n",
    "            x_batch = input_seq[t_start:t_end]\n",
    "\n",
    "            # Reshape for processing: (current_batch_size * batch_size, hidden_dim)\n",
    "            x_batch_flat = x_batch.reshape(-1, self.hidden_dim)\n",
    "\n",
    "            # Get the weights at the start of this mini-batch (W_{t'})\n",
    "            W_start_batch = hidden_states_W[-1] # Use W from end of last batch\n",
    "\n",
    "            # --- Parallel Gradient Calculation (Conceptually) ---\n",
    "            # We need gradients w.r.t W_start_batch for each token in the batch\n",
    "            # Calculate views for the entire flattened batch\n",
    "            # x_train/label/test_flat: (current_batch_size * batch_size, rank)\n",
    "            x_train_flat, x_label_flat, x_test_flat = self.task(x_batch_flat)\n",
    "\n",
    "            # Calculate predictions using the inner model f with W_start_batch\n",
    "            # pred_flat: (current_batch_size * batch_size, rank)\n",
    "            pred_flat = self._inner_model_f(x_train_flat, W_start_batch)\n",
    "\n",
    "            # Calculate loss for the entire batch\n",
    "            # Note: Ensure reduction='none' if we need per-token gradients later,\n",
    "            # but for a single batch update, 'mean' is fine.\n",
    "            loss = F.mse_loss(pred_flat, x_label_flat)\n",
    "\n",
    "            # Calculate the single gradient for the entire batch w.r.t W_start_batch\n",
    "            # This gradient represents the average direction over the mini-batch\n",
    "            grad_W_batch = torch.autograd.grad(loss, W_start_batch, retain_graph=True)[0]\n",
    "\n",
    "            # --- Calculate Learnable Learning Rate (eta) ---\n",
    "            # Calculate eta per token based on original x_batch_flat\n",
    "            # eta_gate_flat: (current_batch_size * batch_size, 1)\n",
    "            eta_gate_flat = torch.sigmoid(self.theta_lr(x_batch_flat))\n",
    "            # Average eta over the batch? Or apply per-token scaling to gradient?\n",
    "            # Paper Eq 6 uses sum(G_s), where G_s = grad(l(W; x_s)).\n",
    "            # Learnable eta (Sec 2.7) eta(x) = eta_base * sigmoid(theta_lr * x).\n",
    "            # Let's assume eta is applied *after* summing gradients, using an average eta.\n",
    "            avg_eta_multiplier = eta_gate_flat.mean()\n",
    "            eta = self.inner_eta_base * avg_eta_multiplier\n",
    "\n",
    "            # --- Single Weight Update for the Batch ---\n",
    "            # Update W for the *next* mini-batch start state\n",
    "            W_end_batch = W_start_batch - eta * grad_W_batch\n",
    "            # Detach and require grad for the next iteration's gradient calculation\n",
    "            W_end_batch = W_end_batch.detach().requires_grad_(True)\n",
    "            hidden_states_W.append(W_end_batch)\n",
    "\n",
    "            # --- Calculate Output for the Batch ---\n",
    "            # Use the *updated* weights (W_end_batch) for prediction.\n",
    "            # This is a simplification. The dual form (Sec 2.5, Appendix A)\n",
    "            # computes the exact output z_t using W_t within the batch efficiently.\n",
    "            # Using W_end_batch applies the average update effect to all tokens in the batch.\n",
    "            # z_flat: (current_batch_size * batch_size, rank)\n",
    "            z_flat = self._inner_model_f(x_test_flat, W_end_batch)\n",
    "\n",
    "            # Reshape output back to (current_batch_size, batch_size, rank)\n",
    "            z_batch = z_flat.reshape(current_batch_size, batch_size, self.projection_rank)\n",
    "            outputs.append(z_batch)\n",
    "\n",
    "        # Concatenate outputs from all mini-batches\n",
    "        # output_seq_rank: (seq_len, batch_size, rank)\n",
    "        output_seq_rank = torch.cat(outputs, dim=0)\n",
    "\n",
    "        # Project back to hidden_dim - ADDED based on re-evaluation\n",
    "        # If TTTLayer is meant to replace a standard RNN/Attention layer,\n",
    "        # it should likely preserve the hidden dimension.\n",
    "        # Create this projection lazily if it doesn't exist\n",
    "        if not hasattr(self, 'output_proj'):\n",
    "             self.output_proj = nn.Linear(self.projection_rank, self.hidden_dim, bias=False).to(device)\n",
    "\n",
    "        output_seq = self.output_proj(output_seq_rank)\n",
    "\n",
    "        return output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTTModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Language Model using TTT layers, incorporating a Transformer-like backbone.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, inner_eta_base=0.1,\n",
    "                 ttt_batch_size=16, projection_rank=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Token embedding\n",
    "        self.embed_tokens = nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack of TTT Layers interleaved with MLP blocks and LayerNorms\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                nn.LayerNorm(hidden_dim), # Pre-TTT Norm\n",
    "                TTTLayer(hidden_dim,\n",
    "                         inner_eta_base=inner_eta_base,\n",
    "                         ttt_batch_size=ttt_batch_size,\n",
    "                         projection_rank=projection_rank),\n",
    "                nn.LayerNorm(hidden_dim), # Pre-MLP Norm\n",
    "                MLPBlock(hidden_dim, dropout=dropout)\n",
    "            ]))\n",
    "\n",
    "        # Final LayerNorm\n",
    "        self.final_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # LM Head (predicts next token)\n",
    "        self.lm_head = nn.Linear(hidden_dim, vocab_size, bias=False)\n",
    "\n",
    "        # Optional: Tie weights between embedding and LM head\n",
    "        # self.embed_tokens.weight = self.lm_head.weight\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "        # Initialize TTTLayer initial_W specifically?\n",
    "        # It's already initialized in TTTLayer.__init__\n",
    "\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        Forward pass through the TTT-based language model.\n",
    "\n",
    "        Args:\n",
    "            token_ids (torch.Tensor): Input token IDs, shape (seq_len, batch_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits for next token prediction, shape\n",
    "                          (seq_len, batch_size, vocab_size).\n",
    "        \"\"\"\n",
    "        # 1. Embedding\n",
    "        # token_ids: (seq_len, batch_size)\n",
    "        # x: (seq_len, batch_size, hidden_dim)\n",
    "        x = self.embed_tokens(token_ids)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 2. TTT Layers and MLP Blocks\n",
    "        for ttt_norm, ttt_layer, mlp_norm, mlp_block in self.layers:\n",
    "            # Residual connection around TTT Layer\n",
    "            residual = x\n",
    "            x = ttt_norm(x)\n",
    "            x = ttt_layer(x) # TTT layer processes the sequence\n",
    "            x = self.dropout(x)\n",
    "            x = residual + x\n",
    "\n",
    "            # Residual connection around MLP Block\n",
    "            residual = x\n",
    "            x = mlp_norm(x)\n",
    "            x = mlp_block(x)\n",
    "            # Dropout is already inside MLPBlock\n",
    "            x = residual + x\n",
    "\n",
    "        # 3. Final Norm\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # 4. LM Head\n",
    "        # logits: (seq_len, batch_size, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Example Usage\n",
    "# -------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters for illustration.\n",
    "    vocab_size = 10000\n",
    "    hidden_dim = 512\n",
    "    num_layers = 4\n",
    "    inner_eta_base = 0.05 # Base learning rate for inner loop\n",
    "    ttt_batch_size = 16   # Mini-batch size for TTT updates\n",
    "    projection_rank = 128 # Optional: Use lower rank for projections (e.g., hidden_dim // 4)\n",
    "    dropout = 0.1\n",
    "    seq_len = 64 # Increased sequence length\n",
    "    batch_size = 8\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = TTTModel(vocab_size, hidden_dim, num_layers,\n",
    "                     inner_eta_base=inner_eta_base,\n",
    "                     ttt_batch_size=ttt_batch_size,\n",
    "                     projection_rank=projection_rank,\n",
    "                     dropout=dropout)\n",
    "\n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create some dummy token IDs.\n",
    "    # Shape: (seq_len, batch_size)\n",
    "    token_ids = torch.randint(0, vocab_size, (seq_len, batch_size), device=device)\n",
    "\n",
    "    # --- Forward pass ---\n",
    "    print(\"\\nRunning forward pass...\")\n",
    "    try:\n",
    "        with torch.no_grad(): # Use no_grad for inference example\n",
    "             if device == torch.device(\"cuda\"):\n",
    "                 # Use autocast for mixed precision on GPU\n",
    "                 with torch.cuda.amp.autocast():\n",
    "                     logits = model(token_ids)\n",
    "             else:\n",
    "                 logits = model(token_ids)\n",
    "        print(\"Forward pass successful!\")\n",
    "        print(\"Logits shape:\", logits.shape) # Expected: (seq_len, batch_size, vocab_size)\n",
    "\n",
    "        # --- Optional: Check backward pass (requires gradients) ---\n",
    "        # print(\"\\nRunning backward pass (requires gradients)...\")\n",
    "        # model.train()\n",
    "        # optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4) # Example optimizer\n",
    "        # optimizer.zero_grad()\n",
    "        #\n",
    "        # if device == torch.device(\"cuda\"):\n",
    "        #     with torch.cuda.amp.autocast():\n",
    "        #         logits = model(token_ids)\n",
    "        #         # Dummy loss for testing backward pass\n",
    "        #         dummy_loss = logits.mean()\n",
    "        # else:\n",
    "        #     logits = model(token_ids)\n",
    "        #     dummy_loss = logits.mean()\n",
    "        #\n",
    "        # print(\"Dummy Loss:\", dummy_loss.item())\n",
    "        # dummy_loss.backward()\n",
    "        # optimizer.step()\n",
    "        # print(\"Backward pass successful!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during the forward/backward pass: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
